---
title: "Hello, Quarto"
format: html
---

## Markdown

Markdown is an easy to read and write text format:

- It's _plain text_ so works well with version control
- It can be **rendered** into HTML, PDF, and more
- Learn more at: <https://quarto.org/docs/authoring/>

## Code Cell

Here is a Python code cell:

```{python}

import numpy as np
from itertools import product
import pandas as pd
from lifelines import CoxPHFitter

def extract_aj_estimate(data_to_adjust, fixed_time_horizons):
    """
    Python implementation of the R extract_aj_estimate function for Aalen-Johansen estimation.
    
    Parameters:
    data_to_adjust (pd.DataFrame): DataFrame containing survival data
    fixed_time_horizons (list or float): Time points at which to evaluate the survival
    
    Returns:
    pd.DataFrame: DataFrame with Aalen-Johansen estimates
    """
    import numpy as np
    
    # Ensure 'strata' column exists
    if 'strata' not in data_to_adjust.columns:
        data_to_adjust['strata'] = 'default'

    # Ensure fixed_time_horizons is a list
    if not isinstance(fixed_time_horizons, list):
        fixed_time_horizons = [fixed_time_horizons]
    
    # Create a categorical version of reals for stratification
    data_to_adjust = data_to_adjust.copy()
    data_to_adjust['reals_cat'] = pd.Categorical(
        data_to_adjust['reals'], 
        categories=["real_negatives", "real_positives", "real_competing", "real_censored"],
        ordered=True
    )
    
    # Get unique strata values
    strata_values = data_to_adjust['strata'].unique()
    
    # Initialize result dataframes
    results = []
    
    # For each stratum, fit Aalen-Johansen model
    for stratum in strata_values:
        # Filter data for current stratum
        stratum_data = data_to_adjust[data_to_adjust['strata'] == stratum].copy()
        
        # Initialize Aalen-Johansen fitter
        ajf = AalenJohansenFitter()
        
        # Convert reals to numeric for lifelines
        # In lifelines: 0=censored, 1=event of interest, 2=competing event
        event_map = {
            "real_negatives": 0,  # Treat as censored
            "real_positives": 1,  # Event of interest
            "real_competing": 2,  # Competing risk
            "real_censored": 0    # Censored
        }
        
        stratum_data['event_code'] = stratum_data['reals'].map(event_map)
        
        # Fit the model
        ajf.fit(
            stratum_data['times'], 
            stratum_data['event_code'], 
            event_of_interest=1
        )
        
        # Calculate cumulative incidence at fixed time horizons
        for t in fixed_time_horizons:
            # Get cumulative incidence at time t
            # Handle case where t is beyond the data
            try:
                ci_at_t = ajf.cumulative_density_at_times(t)[0]
            except IndexError:
                ci_at_t = ajf.cumulative_density_.iloc[-1] if len(ajf.cumulative_density_) > 0 else 0
            
            # Calculate counts for each state
            n = len(stratum_data)
            
            # For real_positives: use the cumulative incidence
            real_positives_est = ci_at_t
            
            # For real_negatives: use 1 - cumulative incidence
            real_negatives_est = 1 - ci_at_t
            
            # For real_competing: would be obtained from a competing risks model
            # Here we'll use the count of competing events before time t
            real_competing_count = stratum_data[(stratum_data['event_code'] == 2) & 
                                               (stratum_data['times'] <= t)].shape[0]
            real_competing_est = real_competing_count / n if n > 0 else 0
            
            # For real_censored: would be the proportion of censored before time t
            real_censored_count = stratum_data[(stratum_data['event_code'] == 0) & 
                                              (stratum_data['times'] <= t)].shape[0]
            real_censored_est = real_censored_count / n if n > 0 else 0
            
            # Adjust estimates to ensure they sum to 1
            total_est = real_positives_est + real_negatives_est + real_competing_est + real_censored_est
            if total_est > 0:
                real_positives_est /= total_est
                real_negatives_est /= total_est
                real_competing_est /= total_est
                real_censored_est /= total_est
            
            # Create entries for each real state
            states = ["real_negatives", "real_positives", "real_competing", "real_censored"]
            estimates = [real_negatives_est, real_positives_est, real_competing_est, real_censored_est]
            
            for state, estimate in zip(states, estimates):
                results.append({
                    'strata': stratum,
                    'fixed_time_horizon': t,
                    'reals': state,
                    'n': n,
                    'estimate': estimate,
                    'reals_estimate': estimate * n
                })
    
    # Convert to DataFrame
    result_df = pd.DataFrame(results)
    
    # Convert strata to categorical if needed
    result_df['strata'] = pd.Categorical(result_df['strata'])
    
    return result_df

def extract_crude_estimate(data_to_adjust):
    """
    Calculate crude estimates for each group in the data.

    Parameters:
    data_to_adjust (pd.DataFrame): DataFrame containing the data to adjust with columns 'strata', 'reals', and 'fixed_time_horizon'.

    Returns:
    pd.DataFrame: DataFrame with crude estimates for each group.
    """
    # Count occurrences of each group
    grouped = (
        data_to_adjust
        .groupby(["strata", "reals", "fixed_time_horizon"])
        .size()
        .reset_index(name="reals_estimate")  # Equivalent to dplyr::summarise(n())
    )

    # Create a complete grid of all possible combinations
    strata_values = data_to_adjust["strata"].unique()
    reals_values = data_to_adjust["reals"].unique()
    fixed_time_horizon_values = data_to_adjust["fixed_time_horizon"].unique()

    complete_grid = pd.DataFrame(
        list(product(strata_values, reals_values, fixed_time_horizon_values)),
        columns=["strata", "reals", "fixed_time_horizon"]
    )

def add_cutoff_strata(data, by):
    data["strata_probability_threshold"] = pd.cut(
        data["probs"],
        bins=create_breaks_values(data["probs"], "probability_threshold", by),
        include_lowest=True
    )
    data["strata_ppcr"] = pd.qcut(-data["probs"], q=int(1/by), labels=False) + 1
    data["strata_ppcr"] = data["strata_ppcr"].astype(str)
    return data

def create_breaks_values(probs_vec, stratified_by, by):
    if stratified_by != "probability_threshold":
        breaks = np.quantile(probs_vec, np.linspace(1, 0, int(1/by) + 1))
    else:
        breaks = np.round(np.arange(0, 1 + by, by), decimals=len(str(by).split(".")[-1]))
    return breaks

def create_strata_combinations(stratified_by, by):
    if stratified_by == "probability_threshold":
        upper_bound = create_breaks_values(None, "probability_threshold", by)
        lower_bound = np.roll(upper_bound, 1)
        lower_bound[0] = 0
        mid_point = upper_bound - by / 2
        include_lower_bound = lower_bound == 0
        include_upper_bound = upper_bound != 0
        strata = [f"{'[' if lb else '('}{l},{u}{']' if ub else ')'}" for lb, l, u, ub in zip(include_lower_bound, lower_bound, upper_bound, include_upper_bound)]
        chosen_cutoff = upper_bound
    elif stratified_by == "ppcr":
        strata = create_breaks_values(None, "probability_threshold", by)[1:]
        lower_bound = strata - by
        upper_bound = strata + by
        mid_point = upper_bound - by / 2
        include_lower_bound = np.ones_like(strata, dtype=bool)
        include_upper_bound = np.zeros_like(strata, dtype=bool)
        chosen_cutoff = strata
    return pd.DataFrame({
        "strata": strata,
        "lower_bound": lower_bound,
        "upper_bound": upper_bound,
        "mid_point": mid_point,
        "include_lower_bound": include_lower_bound,
        "include_upper_bound": include_upper_bound,
        "chosen_cutoff": chosen_cutoff,
        "stratified_by": stratified_by
    })



def create_breaks_values(probs_vec, stratified_by, by):
    if stratified_by != "probability_threshold":
        breaks = np.quantile(probs_vec, np.linspace(1, 0, int(1/by) + 1))
    else:
        breaks = np.round(np.arange(0, 1 + by, by), decimals=len(str(by).split(".")[-1]))
    return breaks


def create_aj_data_combinations(reference_groups, fixed_time_horizons, stratified_by, by):
    # Create strata combinations
    strata_combinations = pd.concat(
        [create_strata_combinations(x, by) for x in stratified_by], ignore_index=True
    )
    
    reals = pd.Categorical(
        ["real_negatives", "real_positives", "real_competing", "real_censored"],
        categories=["real_negatives", "real_positives", "real_competing", "real_censored"],
        ordered=True
    )
    
    censoring_assumptions = ["excluded", "adjusted"]
    competing_assumptions = ["excluded", "adjusted_as_negative", "adjusted_as_censored"]
    
    combinations = list(product(reference_groups, fixed_time_horizons, reals, censoring_assumptions, competing_assumptions))
    
    df_combinations = pd.DataFrame(combinations, columns=[
        "reference_group", "fixed_time_horizon", "reals", "censoring_assumption", "competing_assumption"
    ])
    
    return df_combinations.merge(strata_combinations, how="cross")

df_time_to_cancer_dx = \
    pd.read_csv(
        "https://raw.githubusercontent.com/ddsjoberg/dca-tutorial/main/data/df_time_to_cancer_dx.csv"
    )

df_time_to_cancer_dx

cox_model = CoxPHFitter()
thin_model = CoxPHFitter()

# Fit the models
cox_model.fit(df_time_to_cancer_dx, duration_col='ttcancer', event_col='cancer', formula='age + famhistory + marker')
thin_model.fit(df_time_to_cancer_dx, duration_col='ttcancer', event_col='cancer', formula='age + marker')

reals_mapping = {
    "censor": 0,
    "diagnosed with cancer": 1,
    "dead other causes": 2
}
df_time_to_cancer_dx['reals'] = df_time_to_cancer_dx['cancer_cr'].map(reals_mapping)

# Predict risks at time 1.5
new_data = df_time_to_cancer_dx.copy()
new_data['ttcancer'] = 1.5
pred_1_5 = 1 - np.exp(-cox_model.predict_expectation(new_data))
pred_thin = 1 - np.exp(-thin_model.predict_expectation(new_data))

# Store probabilities
probs_cox = {
    "thin": pred_thin,
    "full": pred_1_5
}
# Create Aalen-Johansen data combinations
fixed_time_horizons = [1, 3, 5]
stratified_by = ["probability_threshold", "ppcr"]

# Placeholder for create_aj_data_combinations
aj_data_combinations = create_aj_data_combinations(list(probs_cox.keys()), fixed_time_horizons, stratified_by, 0.01)

aj_data_combinations

# Create reference groups
data_to_adjust = pd.DataFrame({
    "reference_group": ["thin"] * len(probs_cox["thin"]) + ["full"] * len(probs_cox["thin"]),
    "probs": np.concatenate([probs_cox["thin"], probs_cox["full"]]),
    "reals": np.concatenate([df_time_to_cancer_dx['reals'], df_time_to_cancer_dx['reals']]),
    "times": np.concatenate([df_time_to_cancer_dx['ttcancer'], df_time_to_cancer_dx['ttcancer']])
})

# # Placeholder for add_cutoff_strata function
data_to_adjust = add_cutoff_strata(data_to_adjust, by=0.01)

data_to_adjust["reals"] = data_to_adjust["reals"].replace({
    0: "real_negatives",
    2: "real_competing",
    1: "real_positives"
})
data_to_adjust["reals"] = pd.Categorical(data_to_adjust["reals"], categories=["real_negatives", "real_competing", "real_positives"], ordered=True)

# Splitting data by reference group
list_data_to_adjust = {k: v for k, v in data_to_adjust.groupby("reference_group")}

# # Define assumption sets
assumption_sets = [
    {"competing": "excluded", "censored": "excluded"},
    {"competing": "adjusted_as_negative", "censored": "adjusted"},
    {"competing": "adjusted_as_censored", "censored": "adjusted"},
    {"competing": "excluded", "censored": "adjusted"},
    {"competing": "adjusted_as_negative", "censored": "excluded"}
]

def update_administrative_censoring(data_to_adjust):
    data_to_adjust = data_to_adjust.copy()  # Ensure we're not modifying the original DataFrame

    data_to_adjust["reals"] = np.select(
        [
            (data_to_adjust["times"] > data_to_adjust["fixed_time_horizon"]) & (data_to_adjust["reals"] == "real_positives"),
            (data_to_adjust["times"] < data_to_adjust["fixed_time_horizon"]) & (data_to_adjust["reals"] == "real_negatives")
        ],
        [
            "real_negatives",  # If time is greater than horizon and was "real_positives", change to "real_negatives"
            "real_censored"    # If time is less than horizon and was "real_negatives", change to "real_censored"
        ],
        default=data_to_adjust["reals"]  # Keep the original value if no condition is met
    )
    return data_to_adjust


def extract_aj_estimate_by_assumptions(data_to_adjust, fixed_time_horizons, 
                                       censoring_assumption="excluded", 
                                       competing_assumption="excluded"):
    if censoring_assumption == "excluded" and competing_assumption == "excluded":
        aj_estimate_data = (
            data_to_adjust
            .assign(fixed_time_horizon=lambda df: df.apply(lambda x: fixed_time_horizons, axis=1))
            .explode("fixed_time_horizon")
            .pipe(update_administrative_censoring)
            .pipe(extract_crude_estimate)
        )
    
    elif censoring_assumption == "excluded" and competing_assumption == "adjusted_as_negative":
        aj_estimate_data_excluded = (
            data_to_adjust
            .assign(fixed_time_horizon=lambda df: df.apply(lambda x: fixed_time_horizons, axis=1))
            .explode("fixed_time_horizon")
            .pipe(update_administrative_censoring)
            .query("reals == 'real_censored'")
            .pipe(extract_crude_estimate)
        )

        aj_estimate_data_adjusted = (
            pd.concat([
                extract_aj_estimate(
                    data_to_adjust
                    .assign(fixed_time_horizon=h)
                    .query("reals != 'real_censored' and fixed_time_horizon == @h"),
                    fixed_time_horizons=h
                )
                for h in fixed_time_horizons
            ])
            .reset_index(drop=True)
        )

        aj_estimate_data = pd.concat([aj_estimate_data_excluded, aj_estimate_data_adjusted], ignore_index=True)
    
    elif censoring_assumption == "adjusted" and competing_assumption == "excluded":
        aj_estimate_data_excluded = (
            data_to_adjust
            .assign(fixed_time_horizon=lambda df: df.apply(lambda x: fixed_time_horizons, axis=1))
            .explode("fixed_time_horizon")
            .pipe(update_administrative_censoring)
            .query("reals == 'real_competing'")
            .pipe(extract_crude_estimate)
        )

        aj_estimate_data_adjusted = (
            pd.concat([
                extract_aj_estimate(
                    data_to_adjust
                    .assign(fixed_time_horizon=h)
                    .query("reals != 'real_competing' and fixed_time_horizon == @h"),
                    fixed_time_horizons=h
                )
                for h in fixed_time_horizons
            ])
            .reset_index(drop=True)
        )

        aj_estimate_data = pd.concat([aj_estimate_data_excluded, aj_estimate_data_adjusted], ignore_index=True)
    
    elif censoring_assumption == "adjusted" and competing_assumption == "adjusted_as_negative":
        aj_estimate_data = extract_aj_estimate(data_to_adjust, fixed_time_horizons=fixed_time_horizons)
    
    elif censoring_assumption == "adjusted" and competing_assumption == "adjusted_as_censored":
        aj_estimate_data = extract_aj_estimate(
            data_to_adjust.assign(reals=data_to_adjust["reals"].replace({"real_competing": "real_negatives"})),
            fixed_time_horizons=fixed_time_horizons
        )

    return aj_estimate_data.assign(
        censoring_assumption=censoring_assumption,
        competing_assumption=competing_assumption
    )

# # Adjust data based on assumptions
# adjusted_data_list = []
# for reference_group, group_data in list_data_to_adjust.items():
#     for assumptions in assumption_sets:
#         print(f"Processing assumptions: {assumptions}")
#         adjusted_data = extract_aj_estimate_by_assumptions(
#             group_data,
#             fixed_time_horizons=fixed_time_horizons,
#             censoring_assumption=assumptions["censored"],
#             competing_assumption=assumptions["competing"]
#         )
#         adjusted_data["reference_group"] = reference_group
#         adjusted_data_list.append(adjusted_data)

# # Combine all adjusted data
# final_adjusted_data = pd.concat(adjusted_data_list, ignore_index=True)

list_data_to_adjust

```


