---
title: "Getting Started with rtichoke"
---

This tutorial provides an introduction to the `rtichoke` library, showing how to visualize model performance for different scenarios.

## 1. Import Libraries

First, let's import the necessary libraries. We'll need `numpy` for data manipulation and `rtichoke` for the core functionality.

```python
import numpy as np
import rtichoke as rk

# For reproducibility
np.random.seed(42)
```

## 2. Understanding the Inputs

`rtichoke` expects two main inputs for creating performance curves:

*   **`probs` (Probabilities)**: A dictionary where keys are model or population names and values are lists or NumPy arrays of predicted probabilities.
*   **`reals` (Outcomes)**: A dictionary where keys are population names and values are lists or NumPy arrays of the true binary outcomes (0 or 1).

Let's look at the three main use cases.

### Use Case 1: Single Model

This is the simplest case, where you want to evaluate the performance of a single predictive model.

For this, you provide `probs` with a single entry for your model and `reals` with a single entry for the corresponding outcomes.

```python
# Generate realistic sample data for a "good" model
probs_positive_class = np.random.rand(50) * 0.5 + 0.5
probs_negative_class = np.random.rand(50) * 0.5
probs_combined = np.concatenate([probs_positive_class, probs_negative_class])
reals_combined = np.concatenate([np.ones(50), np.zeros(50)])
shuffle_index = np.random.permutation(100)

probs_single = {"Good Model": probs_combined[shuffle_index]}
reals_single = {"Population": reals_combined[shuffle_index]}

# Create a ROC curve
fig = rk.create_roc_curve(
    probs=probs_single,
    reals=reals_single,
)

# In an interactive environment (like a Jupyter notebook),
# this will display the plot.
fig.show()
```

### Use Case 2: Models Comparison

Often, you want to compare the performance of several different models on the *same* population.

For this, you provide `probs` with an entry for each model you want to compare. `reals` will still have a single entry, since the outcome data is the same for all models.

```python
# Generate data for a "Good Model", a "Bad Model", and a "Random Guess"
# The "Good Model" has a clearer separation of probabilities.
good_probs_pos = np.random.rand(50) * 0.4 + 0.6  # 0.6 to 1.0
good_probs_neg = np.random.rand(50) * 0.4       # 0.0 to 0.4
good_probs = np.concatenate([good_probs_pos, good_probs_neg])

# The "Bad Model" has more overlap.
bad_probs_pos = np.random.rand(50) * 0.5 + 0.4  # 0.4 to 0.9
bad_probs_neg = np.random.rand(50) * 0.5 + 0.1  # 0.1 to 0.6
bad_probs = np.concatenate([bad_probs_pos, bad_probs_neg])

reals_comparison_data = np.concatenate([np.ones(50), np.zeros(50)])
shuffle_index_comp = np.random.permutation(100)

probs_comparison = {
    "Good Model": good_probs[shuffle_index_comp],
    "Bad Model": bad_probs[shuffle_index_comp],
    "Random Guess": np.random.rand(100)
}
reals_comparison = {"Population": reals_comparison_data[shuffle_index_comp]}


# Create a precision-recall curve to compare the models
fig = rk.create_precision_recall_curve(
    probs=probs_comparison,
    reals=reals_comparison,
)

fig.show()
```

### Use Case 3: Several Populations

This is useful when you want to evaluate a single model's performance across different populations. A common example is comparing performance on a training set versus a testing set to check for overfitting.

For this, you provide `probs` with an entry for each population and `reals` with a corresponding entry for each population's outcomes.

```python
# Generate sample data for a train and test set.
# Let's assume the model is slightly overfit, performing better on the train set.

# Train set: clear separation
train_probs_pos = np.random.rand(50) * 0.4 + 0.6
train_probs_neg = np.random.rand(50) * 0.4
train_probs = np.concatenate([train_probs_pos, train_probs_neg])
train_reals = np.concatenate([np.ones(50), np.zeros(50)])
train_shuffle = np.random.permutation(100)

# Test set: more overlap
test_probs_pos = np.random.rand(40) * 0.5 + 0.4
test_probs_neg = np.random.rand(40) * 0.5 + 0.1
test_probs = np.concatenate([test_probs_pos, test_probs_neg])
test_reals = np.concatenate([np.ones(40), np.zeros(40)])
test_shuffle = np.random.permutation(80)

probs_populations = {
    "Train": train_probs[train_shuffle],
    "Test": test_probs[test_shuffle]
}
reals_populations = {
    "Train": train_reals[train_shuffle],
    "Test": test_reals[test_shuffle]
}

# Create a calibration curve to compare the model's performance
# on the two populations.
fig = rk.create_calibration_curve(
    probs=probs_populations,
    reals=reals_populations,
)

fig.show()
```

And that's it! You've now seen how to create three of the most common evaluation plots with `rtichoke`. From here, you can explore the other curve types and options that the library has to offer in the [API Reference](../reference/index.qmd).
